"""
LangGraph å®æˆ˜æ¡ˆä¾‹ï¼šç ”ç©¶åŠ©æ‰‹
ç»¼åˆè¿ç”¨ï¼šAgentèŠ‚ç‚¹ + æ¡ä»¶åˆ†æ”¯ + å¾ªç¯ + Human-in-the-loop
"""

from typing import TypedDict, Annotated
from datetime import datetime
from dotenv import load_dotenv
import operator
import os

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

load_dotenv()

# åˆå§‹åŒ–
llm = ChatOpenAI(
    base_url=os.getenv("OPENAI_API_BASE"),
    api_key=os.getenv("OPENAI_API_KEY"),
    model="deepseek-ai/DeepSeek-V3",
    temperature=0,
)

search_tool = TavilySearchResults(max_results=3)

class GraphState(TypedDict):
    # ä½ æ¥å®šä¹‰ 6 ä¸ªå­—æ®µ
    question:str
    search_result:Annotated[str, operator.add]
    search_count:int
    is_sufficient:bool
    report:str
    final_output:str

def search_node(state:GraphState)->dict:
    question = state["question"]
    count = state["search_count"]
    results = search_tool.invoke({"query": question})
    formatted = f"\n--- ç¬¬{count+1}æ¬¡æœç´¢ ---\n"
    for r in results:
        formatted += r["content"] + "\n"
    return {"search_result": formatted, "search_count": count + 1}

def evaluate_node(state:GraphState)->dict:
    question = state["question"]
    search_result = state["search_result"]
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åŠ©æ‰‹ã€‚

    ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š{question}

    æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼š
    {search_result}

    è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œè¿”å›å½“å‰çš„æ”¶é›†ä¿¡æ¯æ˜¯å¦å……è¶³å¯¹äºå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä½ åªèƒ½
    å›ç­”â€œYESâ€ æˆ–è€… â€œNOâ€æ¥è¡¨æ˜æ˜¯å¦å……è¶³ï¼Œä¸å¯ä»¥è¿”å›ä»»ä½•å…¶ä»–ä¸œè¥¿
    """
    response = llm.invoke([HumanMessage(content=prompt)])
    answer = response.content.strip().upper()
    is_sufficient = "YES" in answer
    return {"is_sufficient":is_sufficient}

def generate_report(state:GraphState)->dict:
    question = state["question"]
    search_result = state["search_result"]
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åŠ©æ‰‹ã€‚

    ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š{question}

    æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼š
    {search_result}

    è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œå†™ä¸€ä»½ç®€æ´çš„ç ”ç©¶æŠ¥å‘Šï¼ˆ300å­—ä»¥å†…ï¼‰ã€‚
    è¦æ±‚ï¼šæœ‰æ ‡é¢˜ã€æœ‰è¦ç‚¹ã€æœ‰æ€»ç»“ã€‚
    """
    response = llm.invoke([HumanMessage(content=prompt)])
    return {"report":response.content}
def format_output(state:GraphState)->dict:
    report = state["report"]
    search_count = state["search_count"]
    question = state["question"]
    final_output = f"æŠ¥å‘Šï¼š{report}ï¼Œæœç´¢æ¬¡æ•°ï¼š{search_count}ï¼Œé—®é¢˜ï¼š{question}"
    return {"final_output":final_output}

def should_continue(state: GraphState) -> str:
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­å¾ªç¯"""
    # ä½ çš„ä»£ç 
    is_sufficient = state["is_sufficient"]
    search_count = state["search_count"]

    if search_count < 3 and not is_sufficient :
        return "search_more"
    return "sufficient"
    
graph = StateGraph(GraphState)

# æ·»åŠ  4 ä¸ªèŠ‚ç‚¹
graph.add_node("search_node", search_node)
graph.add_node("evaluate_node", evaluate_node)
graph.add_node("generate_report", generate_report)
graph.add_node("format_output", format_output)

# å…¥å£
graph.set_entry_point("search_node")

# è¾¹ï¼šsearch â†’ evaluateï¼ˆå›ºå®šï¼‰
graph.add_edge("search_node", "evaluate_node")

# æ¡ä»¶è¾¹ï¼ševaluate â†’ åˆ¤æ–­ â†’ search_more æˆ– sufficient
graph.add_conditional_edges(
    "evaluate_node",
    should_continue,
    {
        "search_more": "search_node",
        "sufficient": "generate_report"
    }
)

# è¾¹ï¼šgenerate_report â†’ format_output â†’ END
graph.add_edge("generate_report", "format_output")
graph.add_edge("format_output", END)

# ç¼–è¯‘ï¼ˆå¸¦ä¸­æ–­å’Œæ£€æŸ¥ç‚¹ï¼‰
memory = MemorySaver()
app = graph.compile(
    checkpointer=memory,
    interrupt_after=["generate_report"] # åœ¨å“ªä¸ªèŠ‚ç‚¹åä¸­æ–­ï¼Ÿ
)

if __name__ == "__main__":
    print("=" * 50)
    print("ç ”ç©¶åŠ©æ‰‹")
    print("=" * 50)

    config = {"configurable": {"thread_id": "research_1"}}

    initial_state = {
        "question": "è¯·å¯¹æ¯”åˆ†æ2026å¹´æ˜¥èŠ‚æ¡£ç¥¨æˆ¿å‰ä¸‰åç”µå½±çš„å£ç¢‘ã€ç¥¨æˆ¿èµ°åŠ¿å’Œè¥é”€ç­–ç•¥çš„å·®å¼‚",
        "search_result": "",
        "search_count": 0,
        "is_sufficient": False,
        "report": "",
        "final_output": ""
    }

    # ç¬¬ä¸€é˜¶æ®µï¼šæœç´¢ + è¯„ä¼° + ç”ŸæˆæŠ¥å‘Šï¼ˆè‡ªåŠ¨å¾ªç¯ï¼‰
    print("\nğŸ” å¼€å§‹ç ”ç©¶...\n")
    result = app.invoke(initial_state, config=config)

    # ä¸­æ–­ï¼šæ˜¾ç¤ºæŠ¥å‘Šç»™ç”¨æˆ·å®¡æŸ¥
    print("\n" + "=" * 50)
    print("ğŸ“ ç ”ç©¶æŠ¥å‘Šè‰ç¨¿ï¼š")
    print("=" * 50)
    print(result["report"])

    print("\n" + "=" * 50)
    user_input = input("ä¿®æ”¹æŠ¥å‘Šï¼Ÿ(å›è½¦è·³è¿‡): ")

    if user_input.strip():
        app.update_state(config, {"report": user_input})
        print("âœ… æŠ¥å‘Šå·²æ›´æ–°")
    else:
        print("âœ… ä½¿ç”¨åŸå§‹æŠ¥å‘Š")

    # ç»§ç»­æ‰§è¡Œ format_output
    print("\nğŸ”„ ç”Ÿæˆæœ€ç»ˆè¾“å‡º...\n")
    final = app.invoke(None, config=config)

    print("\n" + "=" * 50)
    print("ğŸ“‹ æœ€ç»ˆè¾“å‡ºï¼š")
    print("=" * 50)
    print(final["final_output"])